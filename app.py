import streamlit as st
import json
import requests
import urllib.parse
import re
import anthropic
            
from langchain_anthropic.chat_models import ChatAnthropic as ChatAnthropicModel
from langchain.schema import HumanMessage
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from custom_json_loader import JSONLoader

def clean_text(text):
    # <!HS>ì™€ <!HE> íƒœê·¸ ì œê±°
    text = re.sub(r"<!HS>|<!HE>", "", text)
    # <h4> íƒœê·¸ì™€ ê·¸ ë‚´ìš©ì„ ì œê±°
    text = re.sub(r'<h4[^>]*>.*?</h4>', '', text, flags=re.DOTALL)
    # ì—°ì†ëœ ê°œí–‰ ë¬¸ìë¥¼ í•˜ë‚˜ì˜ ê°œí–‰ìœ¼ë¡œ ì¶•ì†Œ
    text = re.sub(r'\n+', '\n', text)
    return text

def fetch_news_data(query, limit):
    # ì¿¼ë¦¬ ì¸ì½”ë”©
    # encoded_query = urllib.parse.quote(query)
    # API URL êµ¬ì„±
    url = f"https://searchapi.news.sbs.co.kr/search/news?query={query}&collection=news_sbs&offset=0&limit={limit}"

    # API ìš”ì²­
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()  # JSON ë°ì´í„° íŒŒì‹±
        articles = data.get('news_sbs', [])
        
        # í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        extracted_data = []
        for article in articles:
            title = clean_text(article.get('TITLE', ''))
            article_text = clean_text(article.get('REDUCE_CONTENTS', ''))
            article_date = clean_text(article.get('DATE', ''))
            link = f"https://news.sbs.co.kr/news/endPage.do?news_id={article.get('DOCID')}"
            
            if title:  # ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                extracted_data.append({
                    'title': title,
                    'article': article_text,
                    'date': article_date,
                    'link': link
                })
        return extracted_data
    else:
        return f"Error fetching data: {response.status_code}"

def format_response(response):
    # ì‘ë‹µì—ì„œ ì‰¼í‘œ, ì , ê·¸ë¦¬ê³  ë‹¤ë¥¸ êµ¬ë¶„ìë“¤ì„ ì œê±°
    cleaned_response = re.sub(r'[^\w\s]', '', response)
    words = cleaned_response.split()
    filtered_words = [word for word in words if word not in ('í‚¤ì›Œë“œ', 'ê²€ìƒ‰ì–´')]
    # ì²« ë‘ ê°œì˜ ë‹¨ì–´ë§Œ ì„ íƒ
    if len(filtered_words) >= 2:
        return '%20'.join(filtered_words[:2])
    elif len(filtered_words) == 1:
        return filtered_words[0]
    return ""

def load_and_split_documents(file_path):
    # JSONLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ JSON íŒŒì¼ ë¡œë“œ
    loader = JSONLoader(file_path=file_path, text_content=False)
    docs = loader.load()
    
    # ê° Document ê°ì²´ì˜ metadataì— source ì •ë³´ ì¶”ê°€
    for doc in docs:
        content = json.loads(doc.page_content)
        doc.metadata['source'] = content['link']
    
    # RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=['\n\n', '\n', ' ', '']
    )
    
    split_docs = []
    for doc in docs:
        content = json.loads(doc.page_content)
        splits = text_splitter.split_text(content['article'])
        
        for i, split in enumerate(splits):
            metadata = {
                'source': doc.metadata['source'],
                'seq_num': doc.metadata['seq_num'],
                'split_idx': i,
                'title': content['title']
            }
            split_docs.append(Document(page_content=split, metadata=metadata))
    
    return split_docs

def create_vector_store(documents):
    # OpenAI ì„ë² ë”© ìƒì„±
    embeddings = OpenAIEmbeddings(openai_api_key=st.secrets["OPENAI_API_KEY"], model="text-embedding-3-small")
    
    if not documents:
        return None
    
    # FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vector_store = FAISS.from_documents(documents, embeddings)
    
    return vector_store

def retrieve_relevant_documents(query, vector_store, k=5):
    # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = vector_store.similarity_search(query, k=k)
    
    return relevant_docs

#########################################################################################

# ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ì—¬ë¶€ í™•ì¸
is_mobile = "mobile" in st.query_params

# ë””ë°”ì´ìŠ¤ì— ë”°ë¼ ë‹¤ë¥¸ íƒ€ì´í‹€ ì¶œë ¥
if is_mobile:
    st.title("ğŸ¸ì…”í‹€ì½•")
else:
    st.title("ğŸ¸ì…”í‹€ì½• â€• ìƒˆë¡œìš´ AI ì§€ì‹ê²€ìƒ‰")

# ì‚¬ìš©ìì™€ ì±—ë´‡ ê°„ì˜ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ session stateë¥¼ í™•ì¸í•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”? 'ì…”í‹€ì½•'ì€ ë¯¿ì„ ìˆ˜ ìˆëŠ” SBS ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ ë§¥ë½ì„ ì½•~ ì§šì–´ ì´í•´í•˜ê³  ë‹µë³€í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ìƒì„±í•œ ë¬¸ì¥ë§ˆë‹¤ ì¶œì²˜ë¥¼ í‘œì‹œí•´ ì‹ ë¢°ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.\n\nì œê°€ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
    ]

# session stateì— ì €ì¥ëœ ëª¨ë“  ë©”ì‹œì§€ë¥¼ ë°˜ë³µí•˜ì—¬ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤.
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ìë¡œë¶€í„° ìƒˆë¡œìš´ ì…ë ¥ì„ ë°›ëŠ” í•„ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ì§ˆë¬¸ì„ placeholderë¡œ ì œì‹œí•©ë‹ˆë‹¤.
if prompt := st.chat_input(placeholder="ìµœê·¼ ì‚¼ì„±ì „ì ì‹¤ì ì„ ì•Œë ¤ì¤˜."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    with st.spinner('sbs ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤..'):
        anthropic_api_key = st.secrets["CLAUDE_API_KEY"]
        llm = ChatAnthropicModel(model="claude-3-haiku-20240307", temperature=0.0, max_tokens=64, anthropic_api_key=anthropic_api_key)

        search_prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

        ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ SBS ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
        1. ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œì™€ ê´€ë ¨ëœ 1-2ê°œì˜ í‚¤ì›Œë“œ ì„ ì •
        2. ê°€ëŠ¥í•œ ê³ ìœ ëª…ì‚¬ ìœ„ì£¼ë¡œ ì„ ë³„, ì—†ìœ¼ë©´ ì¤‘ìš” ì¼ë°˜ëª…ì‚¬ ì„ íƒ
        3. í‚¤ì›Œë“œ 2ê°œë©´ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„

        ì¶œë ¥ í˜•ì‹: í‚¤ì›Œë“œ1 í‚¤ì›Œë“œ2
        """.strip()

        formatted_prompt = llm.invoke([HumanMessage(content=search_prompt)])
        search_keyword = format_response(formatted_prompt.content)

        # st.write("í›„ì²˜ë¦¬ í‚¤ì›Œë“œ: " + search_keyword)
        
        fetched_news_data = fetch_news_data(search_keyword, limit=40)

        # fetched_news_dataë¥¼ ì§ì ‘ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜
        docs = [Document(page_content=article['article'], metadata={'title': article['title'], 'date': article['date'], 'source': article['link']}) for article in fetched_news_data]
        
        # ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        split_docs = text_splitter.split_documents(docs)
        
        # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vector_store = create_vector_store(split_docs)
        
        if vector_store is None:
            empty_answer = "ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ìœ¼ë¡œ ê²€ìƒ‰ëœ sbs ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            st.session_state.messages.append({"role": "assistant", "content": empty_answer})
            st.write(empty_answer)
            skip_answer = True
        else:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë° í†µí•©
            relevant_docs = retrieve_relevant_documents(prompt, vector_store)

            # ê´€ë ¨ ë¬¸ì„œì— ë²ˆí˜¸ ë¶™ì´ê¸°
            numbered_docs = [f"{doc.page_content} (ê¸°ì‚¬ ì‘ì„±ë‚ ì§œ: {doc.metadata['date']}) ({i+1})" for i, doc in enumerate(relevant_docs)]
            
            # ê´€ë ¨ ë¬¸ì„œ í•©ì¹˜ê¸°
            combined_docs = "\n".join(numbered_docs)
            skip_answer = False

    if not skip_answer:
        # ì±—ë´‡ìœ¼ë¡œë¶€í„°ì˜ ì‘ë‹µì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ Streamlit ì»¨í…Œì´ë„ˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        with st.chat_message("assistant"):
                
            # ìµœì¢… ë‹µë³€ ìƒì„±
            answer_prompt = f"""
            ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

            ê´€ë ¨ ë‰´ìŠ¤ ì •ë³´:
            {combined_docs}

            ì§€ì‹œì‚¬í•­:
            1. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
            2. ë‹µë³€ì€ í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ëŠ” ë‘ê´„ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            3. ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ê´€ë ¨ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
            4. 'ì˜¤ëŠ˜', 'ì–´ì œ' ë“±ì˜ ìƒëŒ€ì  ë‚ ì§œëŠ” êµ¬ì²´ì ì¸ ë‚ ì§œë¡œ ë³€í™˜í•˜ì„¸ìš”.
            5. ê° ë¬¸ì¥ ëì— ì¶œì²˜ë¥¼ [1], [2] í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”. ì—¬ëŸ¬ ì¶œì²˜ëŠ” [3][4] í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.

            ë‹µë³€:
            """

            client = anthropic.Anthropic(api_key=anthropic_api_key)

            def stream_response():
                with client.messages.stream(
                    max_tokens=2048,
                    messages=[{"role": "user", "content": answer_prompt}],
                    system="You are a helpful assistant who answers in Korean.",
                    model="claude-3-haiku-20240307",
                    temperature=0.0,
                ) as stream:
                    for text in stream.text_stream:
                        yield text

            response = st.write_stream(stream_response())
            st.session_state.messages.append({"role": "assistant", "content": response})

            annotations = re.findall(r'(\[\d+\])', response)
            if annotations:
                source = "ë‰´ìŠ¤ ì¶œì²˜:\n\n"
                unique_annotations = list(set(annotations))  # ì£¼ì„ ì¤‘ë³µ ì œê±°
                for annotation in unique_annotations:
                    doc_index = int(annotation[1:-1]) - 1  # ëŒ€ê´„í˜¸ ì œê±° í›„ ìˆ«ì ì¶”ì¶œ
                    if 0 <= doc_index < len(relevant_docs):
                        doc = relevant_docs[doc_index]
                        source += f"{annotation} {doc.metadata['title']}\n\n{doc.metadata['source']}\n\n"
                st.markdown(source)
                st.session_state.messages.append({"role": "assistant", "content": re.sub(r'<[^>]+>', '', source)})
